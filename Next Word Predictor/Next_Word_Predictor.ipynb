{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Generator : Next Word Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random \n",
    "import pickle\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import LSTM, Dense, Activation\n",
    "from keras.optimizers import RMSprop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_df = pd.read_csv(\"Fake.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = list(text_df.text.values)\n",
    "joined_text = \" \".join(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define number of words that the model will be trained on\n",
    "partial_text = joined_text[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenization\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "tokens = tokenizer.tokenize(partial_text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'000': 0,\n",
       " '059': 1,\n",
       " '1': 2,\n",
       " '10': 3,\n",
       " '100': 4,\n",
       " '11': 5,\n",
       " '12': 6,\n",
       " '13': 7,\n",
       " '140': 8,\n",
       " '15': 9,\n",
       " '16': 10,\n",
       " '1600': 11,\n",
       " '17': 12,\n",
       " '18': 13,\n",
       " '19': 14,\n",
       " '1967': 15,\n",
       " '1987': 16,\n",
       " '1995': 17,\n",
       " '2': 18,\n",
       " '20': 19,\n",
       " '2000': 20,\n",
       " '2012': 21,\n",
       " '2015': 22,\n",
       " '2016': 23,\n",
       " '2016image': 24,\n",
       " '2016this': 25,\n",
       " '2016twitter': 26,\n",
       " '2017': 27,\n",
       " '2017after': 28,\n",
       " '2017all': 29,\n",
       " '2017are': 30,\n",
       " '2017austin': 31,\n",
       " '2017bloomberg': 32,\n",
       " '2017breaking': 33,\n",
       " '2017calm': 34,\n",
       " '2017can': 35,\n",
       " '2017carrollton': 36,\n",
       " '2017comment': 37,\n",
       " '2017could': 38,\n",
       " '2017country': 39,\n",
       " '2017did': 40,\n",
       " '2017director': 41,\n",
       " '2017disney': 42,\n",
       " '2017do': 43,\n",
       " '2017donald': 44,\n",
       " '2017featured': 45,\n",
       " '2017for': 46,\n",
       " '2017former': 47,\n",
       " '2017franken': 48,\n",
       " '2017garland': 49,\n",
       " '2017good': 50,\n",
       " '2017he': 51,\n",
       " '2017here': 52,\n",
       " '2017his': 53,\n",
       " '2017i': 54,\n",
       " '2017in': 55,\n",
       " '2017is': 56,\n",
       " '2017it': 57,\n",
       " '2017make': 58,\n",
       " '2017no': 59,\n",
       " '2017now': 60,\n",
       " '2017npr': 61,\n",
       " '2017outgoing': 62,\n",
       " '2017pic': 63,\n",
       " '2017pirro': 64,\n",
       " '2017republicans': 65,\n",
       " '2017rofl': 66,\n",
       " '2017slaw': 67,\n",
       " '2017so': 68,\n",
       " '2017some': 69,\n",
       " '2017someone': 70,\n",
       " '2017terrill': 71,\n",
       " '2017that': 72,\n",
       " '2017the': 73,\n",
       " '2017there': 74,\n",
       " '2017they': 75,\n",
       " '2017this': 76,\n",
       " '2017three': 77,\n",
       " '2017trump': 78,\n",
       " '2017twitter': 79,\n",
       " '2017wayne': 80,\n",
       " '2017we': 81,\n",
       " '2017what': 82,\n",
       " '2017who': 83,\n",
       " '2017with': 84,\n",
       " '2017wow': 85,\n",
       " '2017yeah': 86,\n",
       " '2017yes': 87,\n",
       " '2017you': 88,\n",
       " '2017your': 89,\n",
       " '2018': 90,\n",
       " '2059more': 91,\n",
       " '21': 92,\n",
       " '23': 93,\n",
       " '24': 94,\n",
       " '25': 95,\n",
       " '25th': 96,\n",
       " '25thamendmentnow': 97,\n",
       " '28': 98,\n",
       " '280': 99,\n",
       " '29': 100,\n",
       " '3': 101,\n",
       " '30': 102,\n",
       " '31': 103,\n",
       " '33': 104,\n",
       " '34': 105,\n",
       " '38': 106,\n",
       " '3m': 107,\n",
       " '3vmzuteylx': 108,\n",
       " '4': 109,\n",
       " '40': 110,\n",
       " '400': 111,\n",
       " '404': 112,\n",
       " '479': 113,\n",
       " '48': 114,\n",
       " '49': 115,\n",
       " '4fpae2kypa': 116,\n",
       " '5': 117,\n",
       " '50': 118,\n",
       " '500': 119,\n",
       " '51': 120,\n",
       " '52': 121,\n",
       " '5gemcjqtbh': 122,\n",
       " '6': 123,\n",
       " '60': 124,\n",
       " '600': 125,\n",
       " '65fhbqhuv4': 126,\n",
       " '69': 127,\n",
       " '7': 128,\n",
       " '70': 129,\n",
       " '700': 130,\n",
       " '71': 131,\n",
       " '74': 132,\n",
       " '75': 133,\n",
       " '7lhykiloyz': 134,\n",
       " '8': 135,\n",
       " '84th': 136,\n",
       " '8tktrmqra1': 137,\n",
       " '9': 138,\n",
       " '90': 139,\n",
       " '9lcqpyujkn': 140,\n",
       " '_cingraham': 141,\n",
       " '_elizabethmay': 142,\n",
       " 'a': 143,\n",
       " 'a45en9jwys': 144,\n",
       " 'aaron': 145,\n",
       " 'ab8ami0uxb': 146,\n",
       " 'abdel': 147,\n",
       " 'abdullah': 148,\n",
       " 'abigail': 149,\n",
       " 'abilities': 150,\n",
       " 'ability': 151,\n",
       " 'able': 152,\n",
       " 'abortion': 153,\n",
       " 'about': 154,\n",
       " 'above': 155,\n",
       " 'abravanel': 156,\n",
       " 'absolutely': 157,\n",
       " 'absurdly': 158,\n",
       " 'abuse': 159,\n",
       " 'accept': 160,\n",
       " 'acceptance': 161,\n",
       " 'access': 162,\n",
       " 'according': 163,\n",
       " 'account': 164,\n",
       " 'accounts': 165,\n",
       " 'accurate': 166,\n",
       " 'accuse': 167,\n",
       " 'accused': 168,\n",
       " 'accuser': 169,\n",
       " 'accusers': 170,\n",
       " 'accusing': 171,\n",
       " 'achieve': 172,\n",
       " 'achievement': 173,\n",
       " 'acknowledged': 174,\n",
       " 'across': 175,\n",
       " 'act': 176,\n",
       " 'acting': 177,\n",
       " 'actions': 178,\n",
       " 'actively': 179,\n",
       " 'activists': 180,\n",
       " 'activities': 181,\n",
       " 'actor': 182,\n",
       " 'actual': 183,\n",
       " 'actually': 184,\n",
       " 'ad': 185,\n",
       " 'add': 186,\n",
       " 'added': 187,\n",
       " 'addicting': 188,\n",
       " 'address': 189,\n",
       " 'adds': 190,\n",
       " 'admin': 191,\n",
       " 'administration': 192,\n",
       " 'admit': 193,\n",
       " 'admitted': 194,\n",
       " 'ado': 195,\n",
       " 'adoration': 196,\n",
       " 'adult': 197,\n",
       " 'advice': 198,\n",
       " 'adviser': 199,\n",
       " 'advisor': 200,\n",
       " 'advocating': 201,\n",
       " 'af': 202,\n",
       " 'affair': 203,\n",
       " 'afghanistan': 204,\n",
       " 'afp': 205,\n",
       " 'afraid': 206,\n",
       " 'african': 207,\n",
       " 'after': 208,\n",
       " 'again': 209,\n",
       " 'against': 210,\n",
       " 'age': 211,\n",
       " 'agencies': 212,\n",
       " 'agency': 213,\n",
       " 'agenda': 214,\n",
       " 'agents': 215,\n",
       " 'agitated': 216,\n",
       " 'ago': 217,\n",
       " 'agree': 218,\n",
       " 'agreed': 219,\n",
       " 'agreeing': 220,\n",
       " 'aide': 221,\n",
       " 'aids': 222,\n",
       " 'al': 223,\n",
       " 'alabama': 224,\n",
       " 'alan': 225,\n",
       " 'alansandoval13': 226,\n",
       " 'alarm': 227,\n",
       " 'albania': 228,\n",
       " 'albino': 229,\n",
       " 'alcohol': 230,\n",
       " 'alec': 231,\n",
       " 'alex': 232,\n",
       " 'alexander': 233,\n",
       " 'alive': 234,\n",
       " 'all': 235,\n",
       " 'allegations': 236,\n",
       " 'alleged': 237,\n",
       " 'allegedly': 238,\n",
       " 'alleging': 239,\n",
       " 'allies': 240,\n",
       " 'allow': 241,\n",
       " 'allowed': 242,\n",
       " 'almost': 243,\n",
       " 'alone': 244,\n",
       " 'along': 245,\n",
       " 'already': 246,\n",
       " 'alsen': 247,\n",
       " 'also': 248,\n",
       " 'alt': 249,\n",
       " 'alt_uscis': 250,\n",
       " 'altercation': 251,\n",
       " 'alternative': 252,\n",
       " 'although': 253,\n",
       " 'altstatedpt': 254,\n",
       " 'always': 255,\n",
       " 'am': 256,\n",
       " 'amazing': 257,\n",
       " 'ambassador': 258,\n",
       " 'amen': 259,\n",
       " 'amend': 260,\n",
       " 'amendment': 261,\n",
       " 'amendments': 262,\n",
       " 'america': 263,\n",
       " 'american': 264,\n",
       " 'american_bridge': 265,\n",
       " 'americans': 266,\n",
       " 'among': 267,\n",
       " 'amount': 268,\n",
       " 'amounts': 269,\n",
       " 'an': 270,\n",
       " 'analogy': 271,\n",
       " 'analysis': 272,\n",
       " 'anarchy': 273,\n",
       " 'anchor': 274,\n",
       " 'and': 275,\n",
       " 'andrew': 276,\n",
       " 'angela': 277,\n",
       " 'anger': 278,\n",
       " 'angerer': 279,\n",
       " 'angry': 280,\n",
       " 'animal': 281,\n",
       " 'animated': 282,\n",
       " 'animatronic': 283,\n",
       " 'animatronics': 284,\n",
       " 'ann': 285,\n",
       " 'anniversary': 286,\n",
       " 'announce': 287,\n",
       " 'announced': 288,\n",
       " 'announcement': 289,\n",
       " 'announcements': 290,\n",
       " 'annual': 291,\n",
       " 'annum': 292,\n",
       " 'anonymity': 293,\n",
       " 'another': 294,\n",
       " 'anthem': 295,\n",
       " 'anti': 296,\n",
       " 'antics': 297,\n",
       " 'antidote': 298,\n",
       " 'antipathy': 299,\n",
       " 'any': 300,\n",
       " 'anymore': 301,\n",
       " 'anyone': 302,\n",
       " 'anything': 303,\n",
       " 'anyway': 304,\n",
       " 'anywhere': 305,\n",
       " 'ap': 306,\n",
       " 'apart': 307,\n",
       " 'apbusiness': 308,\n",
       " 'apocalypse': 309,\n",
       " 'apolitical': 310,\n",
       " 'apology': 311,\n",
       " 'appalling': 312,\n",
       " 'apparatus': 313,\n",
       " 'apparently': 314,\n",
       " 'appearances': 315,\n",
       " 'appearing': 316,\n",
       " 'appears': 317,\n",
       " 'appetite': 318,\n",
       " 'applebees': 319,\n",
       " 'appointed': 320,\n",
       " 'appointing': 321,\n",
       " 'approach': 322,\n",
       " 'appropriate': 323,\n",
       " 'approximately': 324,\n",
       " 'april': 325,\n",
       " 'arc': 326,\n",
       " 'arctic': 327,\n",
       " 'are': 328,\n",
       " 'areas': 329,\n",
       " 'aren': 330,\n",
       " 'argue': 331,\n",
       " 'arizona': 332,\n",
       " 'army': 333,\n",
       " 'around': 334,\n",
       " 'arrange': 335,\n",
       " 'arrest': 336,\n",
       " 'arrested': 337,\n",
       " 'arrive': 338,\n",
       " 'arrived': 339,\n",
       " 'arrogance': 340,\n",
       " 'article': 341,\n",
       " 'as': 342,\n",
       " 'ashamed': 343,\n",
       " 'aside': 344,\n",
       " 'ask': 345,\n",
       " 'asked': 346,\n",
       " 'asking': 347,\n",
       " 'asleep': 348,\n",
       " 'ass': 349,\n",
       " 'assault': 350,\n",
       " 'assaulter': 351,\n",
       " 'assaulting': 352,\n",
       " 'assessment': 353,\n",
       " 'assets': 354,\n",
       " 'asshat': 355,\n",
       " 'asshole': 356,\n",
       " 'assignment': 357,\n",
       " 'associate': 358,\n",
       " 'associated': 359,\n",
       " 'association': 360,\n",
       " 'assume': 361,\n",
       " 'assumption': 362,\n",
       " 'at': 363,\n",
       " 'atlantic': 364,\n",
       " 'atop': 365,\n",
       " 'atrupar': 366,\n",
       " 'attack': 367,\n",
       " 'attacked': 368,\n",
       " 'attacking': 369,\n",
       " 'attempt': 370,\n",
       " 'attempted': 371,\n",
       " 'attempts': 372,\n",
       " 'attend': 373,\n",
       " 'attendance': 374,\n",
       " 'attention': 375,\n",
       " 'attorney': 376,\n",
       " 'audio': 377,\n",
       " 'australian': 378,\n",
       " 'australians': 379,\n",
       " 'authorities': 380,\n",
       " 'authors': 381,\n",
       " 'autocracies': 382,\n",
       " 'autocracy': 383,\n",
       " 'autocrat': 384,\n",
       " 'autocratic': 385,\n",
       " 'autocrats': 386,\n",
       " 'ave': 387,\n",
       " 'aviv': 388,\n",
       " 'avoid': 389,\n",
       " 'aware': 390,\n",
       " 'away': 391,\n",
       " 'awesome': 392,\n",
       " 'awhile': 393,\n",
       " 'ayblgmk65z': 394,\n",
       " 'b': 395,\n",
       " 'back': 396,\n",
       " 'backdoor': 397,\n",
       " 'backdoorrussian': 398,\n",
       " 'backlash': 399,\n",
       " 'backwoods': 400,\n",
       " 'bacteria': 401,\n",
       " 'bad': 402,\n",
       " 'badge': 403,\n",
       " 'badly': 404,\n",
       " 'bafflement': 405,\n",
       " 'bags': 406,\n",
       " 'baked': 407,\n",
       " 'baker': 408,\n",
       " 'baldwin': 409,\n",
       " 'ban': 410,\n",
       " 'band': 411,\n",
       " 'bank': 412,\n",
       " 'bankruptcy': 413,\n",
       " 'banned': 414,\n",
       " 'bannon': 415,\n",
       " 'bar': 416,\n",
       " 'barack': 417,\n",
       " 'barely': 418,\n",
       " 'barinholtz': 419,\n",
       " 'base': 420,\n",
       " 'baseball': 421,\n",
       " 'based': 422,\n",
       " 'basically': 423,\n",
       " 'battalion': 424,\n",
       " 'batting': 425,\n",
       " 'be': 426,\n",
       " 'beach': 427,\n",
       " 'beahm': 428,\n",
       " 'bearer': 429,\n",
       " 'beat': 430,\n",
       " 'beaten': 431,\n",
       " 'beating': 432,\n",
       " 'beauty': 433,\n",
       " 'became': 434,\n",
       " 'because': 435,\n",
       " 'beck': 436,\n",
       " 'become': 437,\n",
       " 'becoming': 438,\n",
       " 'bed': 439,\n",
       " 'beef': 440,\n",
       " 'been': 441,\n",
       " 'before': 442,\n",
       " 'beg': 443,\n",
       " 'began': 444,\n",
       " 'begged': 445,\n",
       " 'begging': 446,\n",
       " 'beginning': 447,\n",
       " 'behalf': 448,\n",
       " 'behaving': 449,\n",
       " 'behavior': 450,\n",
       " 'behind': 451,\n",
       " 'being': 452,\n",
       " 'beliefs': 453,\n",
       " 'believe': 454,\n",
       " 'believed': 455,\n",
       " 'believes': 456,\n",
       " 'belongs': 457,\n",
       " 'below': 458,\n",
       " 'benefit': 459,\n",
       " 'benefits': 460,\n",
       " 'bennet': 461,\n",
       " 'beside': 462,\n",
       " 'besides': 463,\n",
       " 'best': 464,\n",
       " 'bet': 465,\n",
       " 'bethlehem': 466,\n",
       " 'better': 467,\n",
       " 'between': 468,\n",
       " 'beyond': 469,\n",
       " 'big': 470,\n",
       " 'bigger': 471,\n",
       " 'biggest': 472,\n",
       " 'bigoted': 473,\n",
       " 'bigotry': 474,\n",
       " 'bigots': 475,\n",
       " 'bill': 476,\n",
       " 'billboard': 477,\n",
       " 'billion': 478,\n",
       " 'billy': 479,\n",
       " 'binder': 480,\n",
       " 'bipartisan': 481,\n",
       " 'bishop': 482,\n",
       " 'bit': 483,\n",
       " 'bitch': 484,\n",
       " 'bite': 485,\n",
       " 'biter': 486,\n",
       " 'bizarre': 487,\n",
       " 'black': 488,\n",
       " 'blamed': 489,\n",
       " 'bland': 490,\n",
       " 'blasted': 491,\n",
       " 'blasting': 492,\n",
       " 'blatant': 493,\n",
       " 'blessing': 494,\n",
       " 'blew': 495,\n",
       " 'blind': 496,\n",
       " 'block': 497,\n",
       " 'blood': 498,\n",
       " 'blow': 499,\n",
       " 'blowing': 500,\n",
       " 'blue': 501,\n",
       " 'bob': 502,\n",
       " 'bobnde79': 503,\n",
       " 'body': 504,\n",
       " 'boil': 505,\n",
       " 'bomb': 506,\n",
       " 'bombed': 507,\n",
       " 'bombs': 508,\n",
       " 'bombshell': 509,\n",
       " 'book': 510,\n",
       " 'borders': 511,\n",
       " 'born': 512,\n",
       " 'boss': 513,\n",
       " 'bossie': 514,\n",
       " 'both': 515,\n",
       " 'bothering': 516,\n",
       " 'bottom': 517,\n",
       " 'bounds': 518,\n",
       " 'boy': 519,\n",
       " 'boycotts': 520,\n",
       " 'boys': 521,\n",
       " 'brace': 522,\n",
       " 'bragged': 523,\n",
       " 'brain': 524,\n",
       " 'brainiac': 525,\n",
       " 'brand': 526,\n",
       " 'brass': 527,\n",
       " 'brave': 528,\n",
       " 'bread': 529,\n",
       " 'break': 530,\n",
       " 'breaking': 531,\n",
       " 'brendan': 532,\n",
       " 'brennan': 533,\n",
       " 'bridge': 534,\n",
       " 'brief': 535,\n",
       " 'bring': 536,\n",
       " 'bringing': 537,\n",
       " 'brings': 538,\n",
       " 'broad': 539,\n",
       " 'brohibition': 540,\n",
       " 'broke': 541,\n",
       " 'brought': 542,\n",
       " 'brown': 543,\n",
       " 'bruce': 544,\n",
       " 'brutal': 545,\n",
       " 'brutality': 546,\n",
       " 'brutalize': 547,\n",
       " 'brutalizing': 548,\n",
       " 'brutally': 549,\n",
       " 'bs': 550,\n",
       " 'buddy': 551,\n",
       " 'budget': 552,\n",
       " 'buffoon': 553,\n",
       " 'build': 554,\n",
       " 'bullitt': 555,\n",
       " 'bullshit': 556,\n",
       " 'bully': 557,\n",
       " 'bump': 558,\n",
       " 'bunch': 559,\n",
       " 'burda': 560,\n",
       " 'burdensome': 561,\n",
       " 'bureau': 562,\n",
       " 'burton': 563,\n",
       " 'bush': 564,\n",
       " 'business': 565,\n",
       " 'but': 566,\n",
       " 'butt': 567,\n",
       " 'by': 568,\n",
       " 'bye': 569,\n",
       " 'cabinet': 570,\n",
       " 'cain': 571,\n",
       " 'california': 572,\n",
       " 'call': 573,\n",
       " 'called': 574,\n",
       " 'calling': 575,\n",
       " 'calls': 576,\n",
       " 'calorie': 577,\n",
       " 'calvin': 578,\n",
       " 'calvinstowell': 579,\n",
       " 'came': 580,\n",
       " 'camera': 581,\n",
       " 'cameras': 582,\n",
       " 'campaign': 583,\n",
       " 'campaigned': 584,\n",
       " 'campaigns': 585,\n",
       " 'can': 586,\n",
       " 'cancer': 587,\n",
       " 'candidacy': 588,\n",
       " 'candidate': 589,\n",
       " 'cannot': 590,\n",
       " 'cantblameobamaanymor': 591,\n",
       " 'capacity': 592,\n",
       " 'capcara': 593,\n",
       " 'capital': 594,\n",
       " 'capitalized': 595,\n",
       " 'capitol': 596,\n",
       " 'captioned': 597,\n",
       " 'capture': 598,\n",
       " 'card': 599,\n",
       " 'care': 600,\n",
       " 'career': 601,\n",
       " 'caribbean': 602,\n",
       " 'caricature': 603,\n",
       " 'carmine': 604,\n",
       " 'carmine761': 605,\n",
       " 'carnage': 606,\n",
       " 'carol': 607,\n",
       " 'carolina': 608,\n",
       " 'carolinian': 609,\n",
       " 'carr': 610,\n",
       " 'carved': 611,\n",
       " 'case': 612,\n",
       " 'cases': 613,\n",
       " 'cash': 614,\n",
       " 'castile': 615,\n",
       " 'catch': 616,\n",
       " 'catherine': 617,\n",
       " 'cause': 618,\n",
       " 'celebrate': 619,\n",
       " 'celebrating': 620,\n",
       " 'celebration': 621,\n",
       " 'cell': 622,\n",
       " 'cementing': 623,\n",
       " 'center': 624,\n",
       " 'centerpiece': 625,\n",
       " 'ceo': 626,\n",
       " 'certainly': 627,\n",
       " 'chain': 628,\n",
       " 'chains': 629,\n",
       " 'chairman': 630,\n",
       " 'chamber': 631,\n",
       " 'chance': 632,\n",
       " 'chances': 633,\n",
       " 'change': 634,\n",
       " 'changed': 635,\n",
       " 'changes': 636,\n",
       " 'changing': 637,\n",
       " 'channel': 638,\n",
       " 'chaos': 639,\n",
       " 'character': 640,\n",
       " 'characters': 641,\n",
       " 'charge': 642,\n",
       " 'charged': 643,\n",
       " 'charges': 644,\n",
       " 'charleston': 645,\n",
       " 'charlie': 646,\n",
       " 'check': 647,\n",
       " 'checked': 648,\n",
       " 'checkmatewhen': 649,\n",
       " 'chicken': 650,\n",
       " 'chief': 651,\n",
       " 'child': 652,\n",
       " 'children': 653,\n",
       " 'chip': 654,\n",
       " 'chips': 655,\n",
       " 'chocolate': 656,\n",
       " 'choices': 657,\n",
       " 'choose': 658,\n",
       " 'chopping': 659,\n",
       " 'chose': 660,\n",
       " 'chris': 661,\n",
       " 'chriscjackson': 662,\n",
       " 'chrismohney': 663,\n",
       " 'christian': 664,\n",
       " 'christmas': 665,\n",
       " 'christopher': 666,\n",
       " 'chuck': 667,\n",
       " 'chucky': 668,\n",
       " 'church': 669,\n",
       " 'cia': 670,\n",
       " 'circle': 671,\n",
       " 'circulated': 672,\n",
       " 'circusdrew': 673,\n",
       " 'circusrebel': 674,\n",
       " 'citing': 675,\n",
       " 'citizen': 676,\n",
       " 'citizens': 677,\n",
       " 'city': 678,\n",
       " 'civil': 679,\n",
       " 'cked': 680,\n",
       " 'cking': 681,\n",
       " 'claim': 682,\n",
       " 'claimed': 683,\n",
       " 'claiming': 684,\n",
       " 'claims': 685,\n",
       " 'clarke': 686,\n",
       " 'class': 687,\n",
       " 'classic': 688,\n",
       " 'clean': 689,\n",
       " 'cleansed': 690,\n",
       " 'cleansing': 691,\n",
       " 'clear': 692,\n",
       " 'clearly': 693,\n",
       " 'clinton': 694,\n",
       " 'clip': 695,\n",
       " 'clock': 696,\n",
       " 'close': 697,\n",
       " 'closed': 698,\n",
       " 'closing': 699,\n",
       " 'club': 700,\n",
       " 'cnbc': 701,\n",
       " 'cnn': 702,\n",
       " 'cnnmoney': 703,\n",
       " 'co': 704,\n",
       " 'coal': 705,\n",
       " 'code': 706,\n",
       " 'coded': 707,\n",
       " 'coders': 708,\n",
       " 'coding': 709,\n",
       " 'coexistence': 710,\n",
       " 'coffee': 711,\n",
       " 'cognizant': 712,\n",
       " 'cohen': 713,\n",
       " 'coincidence': 714,\n",
       " 'coke': 715,\n",
       " 'cold': 716,\n",
       " 'colin': 717,\n",
       " 'colleague': 718,\n",
       " 'colleagues': 719,\n",
       " 'collection': 720,\n",
       " 'collins': 721,\n",
       " 'collusion': 722,\n",
       " 'color': 723,\n",
       " 'com': 724,\n",
       " 'come': 725,\n",
       " 'comedian': 726,\n",
       " 'comes': 727,\n",
       " 'comey': 728,\n",
       " 'comfortable': 729,\n",
       " 'comfortably': 730,\n",
       " 'coming': 731,\n",
       " 'commentary': 732,\n",
       " 'comments': 733,\n",
       " 'commission': 734,\n",
       " 'commits': 735,\n",
       " 'committee': 736,\n",
       " 'committees': 737,\n",
       " 'common': 738,\n",
       " 'communications': 739,\n",
       " 'community': 740,\n",
       " 'companies': 741,\n",
       " 'company': 742,\n",
       " 'compared': 743,\n",
       " 'compete': 744,\n",
       " 'complain': 745,\n",
       " 'complete': 746,\n",
       " 'completed': 747,\n",
       " 'completely': 748,\n",
       " 'comply': 749,\n",
       " 'complying': 750,\n",
       " 'composed': 751,\n",
       " 'computer': 752,\n",
       " 'con': 753,\n",
       " 'conceived': 754,\n",
       " 'concern': 755,\n",
       " 'concerned': 756,\n",
       " 'concerns': 757,\n",
       " 'condemnation': 758,\n",
       " 'condemned': 759,\n",
       " 'condition': 760,\n",
       " 'condo': 761,\n",
       " 'conduct': 762,\n",
       " 'conference': 763,\n",
       " 'confirm': 764,\n",
       " 'confirmed': 765,\n",
       " 'confuse': 766,\n",
       " 'congratulate': 767,\n",
       " 'congratulating': 768,\n",
       " 'congratulations': 769,\n",
       " 'congress': 770,\n",
       " 'congresses': 771,\n",
       " 'congressional': 772,\n",
       " 'congressionally': 773,\n",
       " 'connor': 774,\n",
       " 'consciously': 775,\n",
       " 'consent': 776,\n",
       " 'consequence': 777,\n",
       " 'consequences': 778,\n",
       " 'conservatives': 779,\n",
       " 'consider': 780,\n",
       " 'consideration': 781,\n",
       " 'considered': 782,\n",
       " 'consisting': 783,\n",
       " 'conspiracy': 784,\n",
       " 'constant': 785,\n",
       " 'constantly': 786,\n",
       " 'constitution': 787,\n",
       " 'consulate': 788,\n",
       " 'consultation': 789,\n",
       " 'contained': 790,\n",
       " 'contains': 791,\n",
       " 'contemplated': 792,\n",
       " 'contested': 793,\n",
       " 'context': 794,\n",
       " 'continue': 795,\n",
       " 'continued': 796,\n",
       " 'continues': 797,\n",
       " 'contrast': 798,\n",
       " 'contribute': 799,\n",
       " 'contribution': 800,\n",
       " 'contributions': 801,\n",
       " 'control': 802,\n",
       " 'controlled': 803,\n",
       " 'convenient': 804,\n",
       " 'conversation': 805,\n",
       " 'convicted': 806,\n",
       " 'convince': 807,\n",
       " 'cooper': 808,\n",
       " 'cooperating': 809,\n",
       " 'cooperative': 810,\n",
       " 'cop': 811,\n",
       " 'copies': 812,\n",
       " 'cops': 813,\n",
       " 'corey': 814,\n",
       " 'corfman': 815,\n",
       " 'corker': 816,\n",
       " 'corner': 817,\n",
       " 'cornyn': 818,\n",
       " 'corporations': 819,\n",
       " 'correct': 820,\n",
       " 'correctly': 821,\n",
       " 'corrupt': 822,\n",
       " 'corruption': 823,\n",
       " 'cosponsoring': 824,\n",
       " 'costing': 825,\n",
       " 'costs': 826,\n",
       " 'could': 827,\n",
       " 'couldn': 828,\n",
       " 'counsel': 829,\n",
       " 'counterparts': 830,\n",
       " 'counties': 831,\n",
       " 'countless': 832,\n",
       " 'countries': 833,\n",
       " 'country': 834,\n",
       " 'counts': 835,\n",
       " 'county': 836,\n",
       " 'couple': 837,\n",
       " 'coupled': 838,\n",
       " 'courageous': 839,\n",
       " 'course': 840,\n",
       " 'court': 841,\n",
       " 'cover': 842,\n",
       " 'coverage': 843,\n",
       " 'covering': 844,\n",
       " 'covfefe': 845,\n",
       " 'craven': 846,\n",
       " 'crazy': 847,\n",
       " 'created': 848,\n",
       " 'creates': 849,\n",
       " 'credibly': 850,\n",
       " 'creeping': 851,\n",
       " 'crime': 852,\n",
       " 'criminal': 853,\n",
       " 'critic': 854,\n",
       " 'critical': 855,\n",
       " 'critically': 856,\n",
       " 'criticism': 857,\n",
       " 'criticized': 858,\n",
       " 'cronies': 859,\n",
       " 'cronyn': 860,\n",
       " 'crooked': 861,\n",
       " 'crossed': 862,\n",
       " 'crow': 863,\n",
       " 'crowd': 864,\n",
       " 'cruel': 865,\n",
       " 'cruz': 866,\n",
       " 'cuffing': 867,\n",
       " 'cuffs': 868,\n",
       " 'culminated': 869,\n",
       " 'cupboards': 870,\n",
       " 'curious': 871,\n",
       " 'current': 872,\n",
       " 'currently': 873,\n",
       " 'curtail': 874,\n",
       " 'custody': 875,\n",
       " 'cut': 876,\n",
       " 'cuts': 877,\n",
       " 'cutting': 878,\n",
       " 'cycles': 879,\n",
       " 'd': 880,\n",
       " 'dad': 881,\n",
       " 'daddy': 882,\n",
       " 'dale': 883,\n",
       " 'damage': 884,\n",
       " 'damaged': 885,\n",
       " 'damaging': 886,\n",
       " 'dammit': 887,\n",
       " 'damning': 888,\n",
       " 'dan': 889,\n",
       " 'danger': 890,\n",
       " 'dangerous': 891,\n",
       " 'dangerously': 892,\n",
       " 'dangers': 893,\n",
       " 'daniel': 894,\n",
       " 'danpatrick': 895,\n",
       " 'dare': 896,\n",
       " 'dared': 897,\n",
       " 'darnit': 898,\n",
       " 'dash': 899,\n",
       " 'date': 900,\n",
       " 'daughter': 901,\n",
       " 'daughters': 902,\n",
       " 'david': 903,\n",
       " 'davidschmid1': 904,\n",
       " 'davis': 905,\n",
       " 'day': 906,\n",
       " 'days': 907,\n",
       " 'ddale8': 908,\n",
       " 'dead': 909,\n",
       " 'deaf': 910,\n",
       " 'deal': 911,\n",
       " 'dealing': 912,\n",
       " 'dear': 913,\n",
       " 'death': 914,\n",
       " 'deaths': 915,\n",
       " 'debate': 916,\n",
       " 'debt': 917,\n",
       " 'dec': 918,\n",
       " 'decade': 919,\n",
       " 'december': 920,\n",
       " 'decency': 921,\n",
       " 'decent': 922,\n",
       " 'decided': 923,\n",
       " 'decision': 924,\n",
       " 'declared': 925,\n",
       " 'declares': 926,\n",
       " 'declaring': 927,\n",
       " 'decline': 928,\n",
       " 'decompose': 929,\n",
       " 'decreasing': 930,\n",
       " 'dedicated': 931,\n",
       " 'deducting': 932,\n",
       " 'deductions': 933,\n",
       " 'deep': 934,\n",
       " 'deeply': 935,\n",
       " 'defeated': 936,\n",
       " 'defended': 937,\n",
       " 'defines': 938,\n",
       " 'degradation': 939,\n",
       " 'delay': 940,\n",
       " 'deleted': 941,\n",
       " 'delicacies': 942,\n",
       " 'delicious': 943,\n",
       " 'delivered': 944,\n",
       " 'democracies': 945,\n",
       " 'democracy': 946,\n",
       " 'democrat': 947,\n",
       " 'democratic': 948,\n",
       " 'democrats': 949,\n",
       " 'demonstrating': 950,\n",
       " 'dems': 951,\n",
       " 'denied': 952,\n",
       " 'denies': 953,\n",
       " 'dennis': 954,\n",
       " 'dent': 955,\n",
       " 'dentures': 956,\n",
       " 'denying': 957,\n",
       " 'department': 958,\n",
       " 'depicted': 959,\n",
       " 'deplaned': 960,\n",
       " 'deputy': 961,\n",
       " 'derailed': 962,\n",
       " 'describe': 963,\n",
       " 'described': 964,\n",
       " 'deserves': 965,\n",
       " 'designed': 966,\n",
       " 'desire': 967,\n",
       " 'desperate': 968,\n",
       " 'despicable': 969,\n",
       " 'despite': 970,\n",
       " 'destroyed': 971,\n",
       " 'destroying': 972,\n",
       " 'detailing': 973,\n",
       " 'details': 974,\n",
       " 'detained': 975,\n",
       " 'determine': 976,\n",
       " 'determined': 977,\n",
       " 'deutsche': 978,\n",
       " 'development': 979,\n",
       " 'devin': 980,\n",
       " 'devolving': 981,\n",
       " 'diabetes': 982,\n",
       " 'dialogue': 983,\n",
       " 'diaper': 984,\n",
       " 'dick': 985,\n",
       " 'did': 986,\n",
       " 'didn': 987,\n",
       " 'die': 988,\n",
       " 'died': 989,\n",
       " 'diet': 990,\n",
       " 'dietcoketime': 991,\n",
       " 'difference': 992,\n",
       " 'different': 993,\n",
       " 'difficult': 994,\n",
       " 'dig': 995,\n",
       " 'dinner': 996,\n",
       " 'diplomat': 997,\n",
       " 'dipshit': 998,\n",
       " 'dire': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Obtain unique tokens\n",
    "unique_tokens = np.unique(tokens)\n",
    "unique_token_index = {token : idx for idx, token in enumerate(unique_tokens)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of words in each input sequence\n",
    "n_words = 10\n",
    "\n",
    "# Initialize lists to store input sequences and their corresponding next words\n",
    "input_words = [] \n",
    "next_words = []\n",
    "\n",
    "# Iterate over the tokens to create input-output pairs for training the model\n",
    "for i in range(len(tokens) - n_words):\n",
    "    # Extract a sequence of n_words words as input\n",
    "    input_sequence = tokens[i:i+n_words]\n",
    "    # Append the input sequence to the input_words list\n",
    "    input_words.append(input_sequence)\n",
    "    \n",
    "    # Extract the next word after the input sequence\n",
    "    next_word = tokens[i + n_words]\n",
    "    # Append the next word to the next_words list\n",
    "    next_words.append(next_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize arrays to store the input sequences and their corresponding output words in a one-hot encoded format\n",
    "# X will store the input sequences, while y will store the corresponding output words\n",
    "X = np.zeros((len(input_words), n_words, len(unique_tokens)), dtype=bool)\n",
    "y = np.zeros((len(next_words), len(unique_tokens)), dtype=bool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over each input sequence and its corresponding index using enumerate\n",
    "for i, words in enumerate(input_words):\n",
    "    # Iterate over each word in the input sequence and its corresponding index using enumerate\n",
    "    for j, word in enumerate(words):\n",
    "        # Set the corresponding index in X to 1 to indicate the presence of the word in the input sequence\n",
    "        # The index is determined by the unique_token_index dictionary, which maps each word to its index in the unique_tokens list\n",
    "        X[i, j, unique_token_index[word]] = 1\n",
    "    \n",
    "    # Set the corresponding index in y to 1 to indicate the presence of the next word in the output\n",
    "    # The index is determined by the unique_token_index dictionary, which maps each word to its index in the unique_tokens list\n",
    "    y[i, unique_token_index[next_words[i]]] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model implementation and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lilib\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add an LSTM layer with 128 units, expecting input sequences of length n_words and with a vocabulary size of len(unique_tokens)\n",
    "# This layer returns sequences, as indicated by return_sequences=True, which is necessary for the subsequent LSTM layer\n",
    "model.add(LSTM(128, input_shape=(n_words, len(unique_tokens)), return_sequences=True))\n",
    "\n",
    "# Add another LSTM layer with 128 units\n",
    "# This layer does not return sequences, so it will output a single vector representing the final state of the LSTM\n",
    "model.add(LSTM(128))\n",
    "\n",
    "# Add a Dense layer with a number of units equal to the size of the vocabulary (len(unique_tokens))\n",
    "# This layer will output a vector of probabilities for each word in the vocabulary\n",
    "model.add(Dense(len(unique_tokens)))\n",
    "\n",
    "# Add an activation layer using the softmax function to convert the output into a probability distribution over the vocabulary\n",
    "model.add(Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 166ms/step - accuracy: 0.0392 - loss: 7.1099\n",
      "Epoch 2/30\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 168ms/step - accuracy: 0.0465 - loss: 6.7040\n",
      "Epoch 3/30\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 172ms/step - accuracy: 0.0525 - loss: 6.5405\n",
      "Epoch 4/30\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 177ms/step - accuracy: 0.0662 - loss: 6.3046\n",
      "Epoch 5/30\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 202ms/step - accuracy: 0.0858 - loss: 6.0031\n",
      "Epoch 6/30\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 193ms/step - accuracy: 0.1116 - loss: 5.6600\n",
      "Epoch 7/30\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 201ms/step - accuracy: 0.1369 - loss: 5.3694\n",
      "Epoch 8/30\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 216ms/step - accuracy: 0.1604 - loss: 5.0639\n",
      "Epoch 9/30\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 184ms/step - accuracy: 0.1904 - loss: 4.7503\n",
      "Epoch 10/30\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 184ms/step - accuracy: 0.2264 - loss: 4.4240\n",
      "Epoch 11/30\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 229ms/step - accuracy: 0.2607 - loss: 4.1481\n",
      "Epoch 12/30\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 190ms/step - accuracy: 0.3050 - loss: 3.8106\n",
      "Epoch 13/30\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 198ms/step - accuracy: 0.3479 - loss: 3.5027\n",
      "Epoch 14/30\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 174ms/step - accuracy: 0.3880 - loss: 3.2360\n",
      "Epoch 15/30\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 179ms/step - accuracy: 0.4441 - loss: 2.9345\n",
      "Epoch 16/30\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 200ms/step - accuracy: 0.4998 - loss: 2.6306\n",
      "Epoch 17/30\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 187ms/step - accuracy: 0.5419 - loss: 2.3692\n",
      "Epoch 18/30\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 195ms/step - accuracy: 0.5895 - loss: 2.1331\n",
      "Epoch 19/30\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 194ms/step - accuracy: 0.6390 - loss: 1.9352\n",
      "Epoch 20/30\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 190ms/step - accuracy: 0.6833 - loss: 1.7180\n",
      "Epoch 21/30\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 196ms/step - accuracy: 0.7259 - loss: 1.5195\n",
      "Epoch 22/30\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 187ms/step - accuracy: 0.7674 - loss: 1.3347\n",
      "Epoch 23/30\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 183ms/step - accuracy: 0.8086 - loss: 1.1780\n",
      "Epoch 24/30\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 175ms/step - accuracy: 0.8416 - loss: 1.0273\n",
      "Epoch 25/30\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 192ms/step - accuracy: 0.8684 - loss: 0.8892\n",
      "Epoch 26/30\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 200ms/step - accuracy: 0.8983 - loss: 0.7569\n",
      "Epoch 27/30\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 195ms/step - accuracy: 0.9196 - loss: 0.6594\n",
      "Epoch 28/30\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 191ms/step - accuracy: 0.9389 - loss: 0.5506\n",
      "Epoch 29/30\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 191ms/step - accuracy: 0.9488 - loss: 0.4747\n",
      "Epoch 30/30\n",
      "\u001b[1m135/135\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 204ms/step - accuracy: 0.9610 - loss: 0.4002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x25ee1ab2790>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compile the model with categorical cross-entropy loss, RMSprop optimizer with a learning rate of 0.01, and accuracy metric\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=RMSprop(learning_rate=0.01), metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model using the input sequences (X) and their corresponding output words (y)\n",
    "# Specify the batch size, number of epochs, and whether to shuffle the data during training\n",
    "model.fit(X, y, batch_size=128, epochs=30, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save model\n",
    "model.save(\"text_generator.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "#Load model in case it is not done\n",
    "model = load_model(\"text_generator.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction for next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(input_text, n_best):\n",
    "    # Convert input_text to lowercase\n",
    "    input_text = input_text.lower()\n",
    "    \n",
    "    # Initialize an array to store the input sequence in a one-hot encoded format\n",
    "    X = np.zeros((1, n_words, len(unique_tokens)))\n",
    "    \n",
    "    # Iterate over each word in the input text\n",
    "    for i, word in enumerate(input_text.split()):\n",
    "        # Encode each word in the input sequence using one-hot encoding\n",
    "        # Set the corresponding index in X to 1 to indicate the presence of the word in the input sequence\n",
    "        X[0, i, unique_token_index[word]] = 1\n",
    "    \n",
    "    # Make predictions using the model for the input sequence X\n",
    "    predictions = model.predict(X)[0]\n",
    "    \n",
    "    # Get the indices of the top n_best predictions based on their probabilities\n",
    "    top_indices = np.argpartition(predictions, -n_best)[-n_best:]\n",
    "    \n",
    "    # Return the indices of the top n_best predictions\n",
    "    return top_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 614ms/step\n"
     ]
    }
   ],
   "source": [
    "possible = predict_next_word(\"He said that\", 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['president', 'all', 'well', 'a', 'had']\n"
     ]
    }
   ],
   "source": [
    "print([unique_tokens[idx] for idx in possible])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(input_text, text_length, creativity=3):\n",
    "    # Split the input_text into a list of words\n",
    "    word_sequence = input_text.split()\n",
    "    \n",
    "    # Initialize a variable to keep track of the current position in the text\n",
    "    current = 0\n",
    "\n",
    "    # Iterate over the specified text_length to generate the desired length of text\n",
    "    for _ in range(text_length):\n",
    "        # Create a sub-sequence of words from the word_sequence\n",
    "        sub_sequence = \" \".join(tokenizer.tokenize(\" \".join(word_sequence).lower()))[current:current+n_words]\n",
    "\n",
    "        try:\n",
    "            # Try to predict the next word based on the sub-sequence using the predict_next_word function\n",
    "            # Choose one of the top predictions with randomness controlled by the creativity parameter\n",
    "            choice = unique_tokens[random.choice(predict_next_word(sub_sequence, creativity))]\n",
    "        except:\n",
    "            # If an exception occurs (e.g., if predict_next_word returns an empty list),\n",
    "            # choose a random word from the unique_tokens list\n",
    "            choice = random.choice(unique_tokens)\n",
    "\n",
    "        # Append the chosen word to the word_sequence\n",
    "        word_sequence.append(choice)\n",
    "        \n",
    "        # Update the current position for the next sub-sequence\n",
    "        current += 1\n",
    "\n",
    "    # Join the word_sequence to form the generated text\n",
    "    return \" \".join(word_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'He said me'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(\"He said\", 1, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
